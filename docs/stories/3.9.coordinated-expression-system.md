# Story 3.9: Coordinated Expression System - Eyes + Ears + Neck

## Status
Draft

## Story
**As a** software developer,
**I want** a ROS2 orchestrator node coordinating eyes, ears, and neck for synchronized emotional expressions,
**so that** Olaf displays coherent multi-modal emotions.

## Acceptance Criteria

1. **Orchestrator Node:**
   - Python ROS2 node: `ros2/src/orchestrator/ros2_nodes/coordinated_expression.py`
   - Node name: `/olaf/coordinated_expression`

2. **High-Level Expression API:**
   - Subscribes to: `/olaf/expression/set` (custom msg: emotion_type, intensity)
   - Emotion types supported:
     - `neutral`: Eyes neutral, ears centered, neck level
     - `happy`: Eyes wide/bright, ears up, slight head tilt
     - `curious`: Eyes large pupils, ears forward, head tilt + roll
     - `thinking`: Eyes looking up-right, ears back, head tilt up
     - `confused`: Eyes asymmetric, ears asymmetric, slight head shake
     - `sad`: Eyes small pupils, ears down/back, head tilt down
     - `excited`: Eyes very wide, ears alert up, head rapid micro-movements

3. **Multi-Modal Coordination Logic:**
   - For each emotion:
     - Compute eye expression parameters → publish to `/olaf/head/expression`
     - Compute ear positions (4 servos) → publish to `/olaf/ears/position`
     - Compute neck position (pan/tilt/roll) → publish to `/olaf/neck/position`
     - Optional: Use gesture commands for complex movements (e.g., "look-left" for curious)

4. **Emotion Mapping Configurations:**
   - **Happy:**
     - Eyes: Expression type 1 (happy), intensity 4
     - Ears: Both vertical +30°, horizontal centered
     - Neck: Tilt up 10°, roll 0°, pan 0°
   - **Curious:**
     - Eyes: Expression type 2 (curious), intensity 3
     - Ears: Left forward +40°, right centered
     - Neck: Roll 15°, tilt up 15°, pan -20° (look left slightly)
   - **Sad:**
     - Eyes: Expression type 5 (sad), intensity 2
     - Ears: Both vertical -20° (down), horizontal back -30°
     - Neck: Tilt down -15°, roll 0°, pan 0°
   - (Similar mappings for neutral, thinking, confused, excited)

5. **Timing Coordination:**
   - All modality commands (eyes, ears, neck) published simultaneously
   - Smooth transitions: Interpolate positions over 300-500ms (optional enhancement)
   - No "stuttering": Ears and neck move at coordinated speeds

6. **Presence-Aware Behaviors:**
   - Subscribes to `/olaf/events/presence_detected`
   - When presence detected after absence: Automatically trigger "curious" expression (looks at person)
   - When presence lost: Return to "neutral" after 5 seconds
   - Configurable: Enable/disable autonomous reactions via parameter

7. **Testing:**
   - Launch: `ros2 run orchestrator coordinated_expression`
   - Manual test: Publish `happy` emotion → eyes brighten, ears up, head tilts
   - Manual test: Publish `sad` emotion → eyes droop, ears down, head lowers
   - All 7 emotions tested, verified visually
   - Presence test: Approach sensor → Olaf looks at you with curious expression

8. **End-to-End Validation:**
   - Full chain: Expression command → Coordinated expression node → Head driver + Ears/Neck driver → I2C → ESP32s → servos + eyes
   - Latency: Expression command to visible movement <200ms
   - Reliability: 50 expression changes without failure
   - Smooth synchronized movement: Eyes, ears, neck all move together

9. **Code Quality:**
   - File: `coordinated_expression.py`
   - Emotion mapping config: YAML or Python dict
   - Comments explain coordination logic
   - Code committed to `ros2/src/orchestrator/ros2_nodes/`

## Tasks / Subtasks

- [ ] **Task 1: Create SetExpression message** (AC: 2)
  - [ ] Create file: `orchestrator/msg/SetExpression.msg`:
    ```
    string emotion_type    # neutral, happy, curious, thinking, confused, sad, excited
    uint8 intensity        # 1-5 (emotion strength)
    ```
  - [ ] Build messages: `colcon build --packages-select orchestrator`
  - [ ] Verify: `ros2 interface show orchestrator/msg/SetExpression`

- [ ] **Task 2: Create coordinated_expression node skeleton** (AC: 1)
  - [ ] Create file: `ros2_nodes/coordinated_expression.py`
  - [ ] Import dependencies: rclpy, message types
  - [ ] Define class: `CoordinatedExpression(Node)`
  - [ ] Initialize publishers:
    - [ ] `/olaf/head/expression` (Expression msg from Epic 1)
    - [ ] `/olaf/ears/position` (EarsPosition from Story 3.7)
    - [ ] `/olaf/neck/position` (NeckPosition from Story 3.7)

- [ ] **Task 3: Define emotion mapping configuration** (AC: 4)
  - [ ] Create emotion_mappings dictionary:
    - [ ] Key: emotion_type string
    - [ ] Value: dict with eyes, ears, neck parameters
  - [ ] Implement mappings for all 7 emotions:
    - [ ] neutral
    - [ ] happy
    - [ ] curious
    - [ ] thinking
    - [ ] confused
    - [ ] sad
    - [ ] excited
  - [ ] Document parameter ranges in comments

- [ ] **Task 4: Implement expression subscription** (AC: 2)
  - [ ] Subscribe to `/olaf/expression/set` (SetExpression msg)
  - [ ] Callback function:
    - [ ] Extract emotion_type and intensity
    - [ ] Lookup emotion mapping
    - [ ] If not found: Log warning, use neutral
    - [ ] Call coordination function

- [ ] **Task 5: Implement multi-modal coordination** (AC: 3)
  - [ ] Create function: `coordinate_expression(emotion_mapping, intensity)`
  - [ ] Extract eye parameters:
    - [ ] expression_type, base_intensity from mapping
    - [ ] Apply intensity modifier (1-5)
    - [ ] Create Expression message
    - [ ] Publish to `/olaf/head/expression`
  - [ ] Extract ear parameters:
    - [ ] left_h, left_v, right_h, right_v from mapping
    - [ ] Create EarsPosition message
    - [ ] Publish to `/olaf/ears/position`
  - [ ] Extract neck parameters:
    - [ ] pan, tilt, roll from mapping
    - [ ] Create NeckPosition message
    - [ ] Publish to `/olaf/neck/position`
  - [ ] Test: All three messages published simultaneously

- [ ] **Task 6: Implement presence-aware behavior** (AC: 6)
  - [ ] Add parameter: `autonomous_reactions` (default: True)
  - [ ] Subscribe to `/olaf/events/presence_detected` (Empty msg)
  - [ ] Callback: Trigger "curious" expression
  - [ ] Subscribe to `/olaf/head/presence` (PresenceData msg)
  - [ ] Create timer: Check presence state every 1s
  - [ ] Timer callback:
    - [ ] If presence was detected and now lost
    - [ ] Wait 5 seconds
    - [ ] Trigger "neutral" expression
  - [ ] Test: Approach sensor → curious, leave → neutral after 5s

- [ ] **Task 7: Implement emotion mappings** (AC: 4)
  - [ ] **Neutral:**
    - [ ] Eyes: type=0, intensity=2
    - [ ] Ears: [90, 90, 90, 90] (all centered)
    - [ ] Neck: [90, 90, 90] (level)
  - [ ] **Happy:**
    - [ ] Eyes: type=1, intensity=4
    - [ ] Ears: [90, 120, 90, 120] (both up +30°)
    - [ ] Neck: [90, 100, 90] (tilt up +10°)
  - [ ] **Curious:**
    - [ ] Eyes: type=2, intensity=3
    - [ ] Ears: [130, 100, 90, 90] (left forward+up, right centered)
    - [ ] Neck: [70, 105, 105] (pan -20°, tilt +15°, roll +15°)
  - [ ] **Thinking:**
    - [ ] Eyes: type=3, intensity=3
    - [ ] Ears: [70, 90, 70, 90] (both back)
    - [ ] Neck: [90, 110, 90] (tilt up +20°)
  - [ ] **Confused:**
    - [ ] Eyes: type=4, intensity=3
    - [ ] Ears: [110, 95, 75, 85] (asymmetric)
    - [ ] Neck: [85, 90, 95] (slight tilt)
  - [ ] **Sad:**
    - [ ] Eyes: type=5, intensity=2
    - [ ] Ears: [70, 70, 70, 70] (both down+back)
    - [ ] Neck: [90, 75, 90] (tilt down -15°)
  - [ ] **Excited:**
    - [ ] Eyes: type=6, intensity=5
    - [ ] Ears: [90, 135, 90, 135] (alert up +45°)
    - [ ] Neck: [90, 95, 90] (slight up)

- [ ] **Task 8: End-to-end testing** (AC: 7, 8)
  - [ ] Launch all nodes:
    - [ ] head_driver (Story 3.8)
    - [ ] ears_neck_driver (Story 3.7)
    - [ ] coordinated_expression (this story)
  - [ ] Test each emotion:
    ```bash
    ros2 topic pub --once /olaf/expression/set orchestrator/msg/SetExpression \
      "{emotion_type: 'happy', intensity: 4}"
    ```
  - [ ] Verify visually:
    - [ ] Eyes change expression
    - [ ] Ears move to position
    - [ ] Neck tilts/pans/rolls
    - [ ] All movements coordinated (start together)
  - [ ] Measure latency: Command to visible movement (<200ms)
  - [ ] Reliability test: Cycle through all 7 emotions 10 times (70 total changes)

- [ ] **Task 9: Presence integration testing** (AC: 6, 7)
  - [ ] Enable autonomous reactions
  - [ ] Approach sensor:
    - [ ] Verify curious expression triggered
    - [ ] Eyes + ears + neck all coordinate
  - [ ] Leave sensor:
    - [ ] Wait 5 seconds
    - [ ] Verify neutral expression triggered
  - [ ] Repeat 5 times

- [ ] **Task 10: Code cleanup and documentation** (AC: 9)
  - [ ] Add docstrings to all functions
  - [ ] Add inline comments for emotion mappings
  - [ ] Create config file (optional): `config/emotion_mappings.yaml`
  - [ ] Update `ros2_nodes/README.md`:
    - [ ] Coordinated expression usage
    - [ ] SetExpression message API
    - [ ] Emotion types and meanings
    - [ ] Presence-aware behavior
    - [ ] Example commands
  - [ ] Commit code to repository

## Dev Notes

### Prerequisites
From Story 3.7 (ROS2 Ears+Neck Driver):
- ears_neck_driver functional
- Topics: `/olaf/ears/position`, `/olaf/neck/position`

From Story 3.8 (ROS2 Head Driver Presence):
- head_driver enhanced with presence
- Topics: `/olaf/head/expression`, `/olaf/events/presence_detected`

From Epic 1 Story 1.6 (Minimal Orchestrator):
- Pattern for orchestrator nodes established

### Emotion Design Philosophy

**Multi-Modal Coherence:**
- Eyes, ears, neck must all convey same emotion
- Example: "Sad" = droopy eyes + down ears + lowered head
- Inconsistent expression confuses observer

**Intensity Scaling:**
- Intensity 1-5 modulates expression strength
- Eyes: Changes animation intensity parameter
- Ears/Neck: Could scale range of motion (optional)

**Expressive Inspiration:**
- **WALL-E:** Head tilts for curiosity
- **Dogs (Doberman):** Ears express emotion (alert up, relaxed back)
- **Anime:** Exaggerated eye expressions for clarity

### Emotion Mapping Details

**Neutral:**
```python
'neutral': {
    'eyes': {'type': 0, 'intensity': 2},
    'ears': {'left_h': 90, 'left_v': 90, 'right_h': 90, 'right_v': 90},
    'neck': {'pan': 90, 'tilt': 90, 'roll': 90}
}
```

**Happy:**
```python
'happy': {
    'eyes': {'type': 1, 'intensity': 4},
    'ears': {'left_h': 90, 'left_v': 120, 'right_h': 90, 'right_v': 120},
    'neck': {'pan': 90, 'tilt': 100, 'roll': 90}
}
```

**Curious:**
```python
'curious': {
    'eyes': {'type': 2, 'intensity': 3},
    'ears': {'left_h': 130, 'left_v': 100, 'right_h': 90, 'right_v': 90},
    'neck': {'pan': 70, 'tilt': 105, 'roll': 105}
}
```

**Thinking:**
```python
'thinking': {
    'eyes': {'type': 3, 'intensity': 3},
    'ears': {'left_h': 70, 'left_v': 90, 'right_h': 70, 'right_v': 90},
    'neck': {'pan': 90, 'tilt': 110, 'roll': 90}
}
```

**Confused:**
```python
'confused': {
    'eyes': {'type': 4, 'intensity': 3},
    'ears': {'left_h': 110, 'left_v': 95, 'right_h': 75, 'right_v': 85},
    'neck': {'pan': 85, 'tilt': 90, 'roll': 95}
}
```

**Sad:**
```python
'sad': {
    'eyes': {'type': 5, 'intensity': 2},
    'ears': {'left_h': 70, 'left_v': 70, 'right_h': 70, 'right_v': 70},
    'neck': {'pan': 90, 'tilt': 75, 'roll': 90}
}
```

**Excited:**
```python
'excited': {
    'eyes': {'type': 6, 'intensity': 5},
    'ears': {'left_h': 90, 'left_v': 135, 'right_h': 90, 'right_v': 135},
    'neck': {'pan': 90, 'tilt': 95, 'roll': 90}
}
```

### Presence-Aware Behavior Logic

**Rising Edge (Presence Detected):**
```python
def on_presence_detected(self, msg):
    if self.autonomous_reactions:
        self.set_expression('curious', intensity=4)
        self.get_logger().info("Presence detected! Showing curious expression.")
```

**Falling Edge (Presence Lost):**
```python
def presence_monitoring_callback(self):
    # Called every 1 second
    if self.last_presence and not self.current_presence:
        # Presence just lost
        self.presence_lost_time = time.time()

    if self.presence_lost_time and (time.time() - self.presence_lost_time > 5.0):
        # 5 seconds since presence lost
        self.set_expression('neutral', intensity=2)
        self.presence_lost_time = None
```

### Testing Commands

**Manual Expression Tests:**
```bash
# Neutral
ros2 topic pub --once /olaf/expression/set orchestrator/msg/SetExpression \
  "{emotion_type: 'neutral', intensity: 2}"

# Happy
ros2 topic pub --once /olaf/expression/set orchestrator/msg/SetExpression \
  "{emotion_type: 'happy', intensity: 4}"

# Curious
ros2 topic pub --once /olaf/expression/set orchestrator/msg/SetExpression \
  "{emotion_type: 'curious', intensity: 3}"

# Sad
ros2 topic pub --once /olaf/expression/set orchestrator/msg/SetExpression \
  "{emotion_type: 'sad', intensity: 2}"

# Excited
ros2 topic pub --once /olaf/expression/set orchestrator/msg/SetExpression \
  "{emotion_type: 'excited', intensity: 5}"
```

**Cycle Through All Emotions (Bash Script):**
```bash
#!/bin/bash
for emotion in neutral happy curious thinking confused sad excited; do
  echo "Setting expression: $emotion"
  ros2 topic pub --once /olaf/expression/set orchestrator/msg/SetExpression \
    "{emotion_type: '$emotion', intensity: 3}"
  sleep 3
done
```

**Monitor All Topics:**
```bash
# Terminal 1: Monitor head expression
ros2 topic echo /olaf/head/expression

# Terminal 2: Monitor ear position
ros2 topic echo /olaf/ears/position

# Terminal 3: Monitor neck position
ros2 topic echo /olaf/neck/position
```

### Performance Considerations

**Publication Rate:**
- Expression commands: On-demand (not periodic)
- Each expression triggers 3 simultaneous publications (eyes, ears, neck)
- Total ROS2 message load: Low (only on expression change)

**Coordination Timing:**
- All 3 publications happen in same callback (simultaneous)
- ROS2 message delivery: <1ms (local communication)
- I2C driver latency: ~10ms per command
- Total latency: Command to servo movement <200ms

### Future Enhancements (Epic 9)

This story provides foundation for Epic 9: Super Expressive Personality System
- Epic 9 will add:
  - 21 emotional states (vs 7 in Epic 3)
  - Organic randomization (±10-15% variation)
  - Emotional decay (emotions fade over time)
  - Micro-movements (continuous subtle adjustments)
- This story validates the multi-modal coordination architecture

### Troubleshooting

**Expression doesn't change:**
- Check node running: `ros2 node list | grep coordinated_expression`
- Check topic subscription: `ros2 topic info /olaf/expression/set`
- Verify emotion_type spelling (case-sensitive)

**Ears/neck don't move:**
- Check driver nodes running: head_driver, ears_neck_driver
- Verify topics published: `ros2 topic echo /olaf/ears/position`
- Check I2C modules: `sudo i2cdetect -y 1` (should show 0x08, 0x09)

**Presence reactions don't work:**
- Check autonomous_reactions parameter enabled
- Verify presence events: `ros2 topic echo /olaf/events/presence_detected`
- Check subscription: `ros2 topic info /olaf/events/presence_detected`

**Movements not coordinated:**
- Verify all 3 publications in same callback
- Check ROS2 QoS settings (should be default/reliable)
- Monitor topic timing: Use `ros2 topic hz` for each topic

### File Locations

```
ros2/src/orchestrator/
├── msg/
│   └── SetExpression.msg                 # <-- THIS STORY (new)
├── ros2_nodes/
│   └── coordinated_expression.py         # <-- THIS STORY (new)
├── config/
│   └── emotion_mappings.yaml             # <-- THIS STORY (optional)
└── README.md                             # <-- THIS STORY (update)
```

## Dev Agent Record

### Agent Model Used
<!-- To be filled by dev agent -->

### Debug Log References
<!-- To be filled by dev agent -->

### Completion Notes List
<!-- To be filled by dev agent -->

### File List
<!-- To be filled by dev agent -->

## QA Results
<!-- To be filled by QA agent -->

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-08 | v1.0 | Initial story creation from Epic 3 | Sarah (Product Owner) |
